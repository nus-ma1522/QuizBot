## RecapProjection Projection onto Subspaces


### mcq

Okay I promised projection onto subspaces last interactive, so here we go! 

But before projecting onto subspaces, let's recap projection vectors onto vectors first. What is the expression for the projection of a vector $w$ onto $u$?

- $\frac{u\cdot w}{|u|^2}w$
* $\frac{u\cdot w}{|u|^2}u$
- $\frac{u\cdot w}{|w|^2}w$
- $\frac{u\cdot w}{|w|^2}u$

### plain

Let's go on to projecting onto subspaces shall we?

We have a subspace $V$ such that $\{u_1, u_2\}$ forms a basis for $V$. We also have a vector $w$, which we want to project onto the subspace $w$. Let the projection be $w_p$

### plain

A reasonable guess for $w_p$ would be the following:

$$\frac{u_1\cdot w}{|u_1|^2}u_1 + \frac{u_2\cdot w}{|u_2|^2}u_2$$

That is, $w_p$ is equal to ($w$'s projection onto $u_1$) $+$ ($w$'s projection onto $u_2$)

### plain

In the previous interactive, we showed that this formula didn't work for the first desmos below, but it worked for the second!

The only thing we changed was the basis vectors we used for representing the subspace $V$.

### embed

https://www.desmos.com/3d/9vcs8awrit
- Projection onto subspace 1

### embed

https://www.desmos.com/3d/tfj5mhx387
- Projection onto subspace 2

### mcq

I've included the two desmos above if you need to take another look at it. Anyways, why does it work for the second desmos but not the first?

* Because $u_1$ and $u_2$ are orthogonal!

### plain

Exactly! To find the projection of a vector $w$ onto a subspace $V$, it is helpful that we have an orthogonal basis for $V$!

## GramSchmidtProcess Gram-Schmidt Process

### plain

So let's actually do the example with numbers now. We have:

$$u_1 = \left(\begin{array}{c} 2 \\ 1 \\ 2 \\\end{array}\right) \space \space \space u_2 = \left(\begin{array}{c} 1 \\ 2 \\ 2 \\\end{array}\right)$$

### mcq

$u_1$ and $u_2$ are not orthogonal at the moment... so how?

- cry about it
- die lor
* make the basis orthogonal!


### plain

But how do we build a orthogonal basis for a subspace from a non-orthogonal basis?

To find a method, it might be helpful to understand how adding vectors to a basis changes the subspace that it spans.

### mcq

Firstly, what does $\text{span}\{u_1\}$ look like in 3d space?

- A point
- A vector
* A line
- A plane

### mcq

$\text{span}\{u_1\}$ is a one dimensional subspace, so it's a line. Now, what does $\text{span}\{u_1,u_2\}$ look like in 3d space?

- A point
- A vector
- A line
* A plane

### plain

When we add $u_2$ to the basis, the line exapnds into a plane as it is able to explore a second dimension now.

### mcq

Question! Instead of $u_2$, let's consider some $v$. If we add $v$ to the basis, i.e. we consider $\text{span}\{u_1,v\}$, when will $\text{span}\{u_1,v\} = V$? 

(remember that $V = \text{span}\{u_1,u_2\}$ because $u_1$,$u_2$ is the basis for $V$)


- Never
* When $v$ is not in $\text{span}\{u_1\}$
* When $v$ is in $\text{span}\{u_1,u_2\}$
- Always

### plain

If $v$ is in $\text{span}\{u_1\}$, then the subspace won't expand. However, if we choose any vector in $V$ that isn't linearly dependent on $u_1$, then we are able to form any vector that is in $V$!

### plain

Let's go back to the original problem. We are given a subspace $V$ such that $\text{span}\{u_1,u_2\} = V$ (i.e. $u_1$, $u_2$ forms a basis for $V$)

We want to find two vectors $v_1, v_2$ such that they form an orthogonal basis for the same subspace $V$.

### mcq

$v_1$ is quite stragithforward. We can let $v_1 = u_1$.

How about $v_2$, what properties of $v_2$ must hold?

* in $V$
* orthogonal to $v_1$

### dialogue

How do we get such a $v_2$? 

- I think I know!
    - Answer
- I want a hint
    - Hint
    
### plain Hint

Hint: look at this desmos from the previous interactive again

### embed

https://www.desmos.com/geometry/y2ubn4hqts
- Decomposition of w into 100% related and 0% related components
    
### mcq Answer

How do we do it?

* Project $v_2$ onto $v_1$, and then use the orthogonal component!

### plain

Exactly! Now, we have vectors $v_1$ and $v_2$ be orthogonal to one another. 

### plain

Let's work through an example. Our original vectors were

$$u_1 = \left(\begin{array}{c} 2 \\ 1 \\ 2 \\\end{array}\right) \space \space \space u_2 = \left(\begin{array}{c} 1 \\ 2 \\ 2 \\\end{array}\right)$$

We also can get $v_1$ directly by setting it to be the same as $u_1$:
$$v_1 = \left(\begin{array}{c} 2 \\ 1 \\ 2 \\\end{array}\right)$$

### mcq

Now, $v_2$. Firstly, what is the projection of $u_2$ onto $v_1$?

- $\frac{1}{9}\left(\begin{array}{c} 1 \\ 2 \\ 2 \\\end{array}\right)$
- $\frac{8}{9}\left(\begin{array}{c} 1 \\ 2\\ 2 \\\end{array}\right)$
- $\frac{1}{9}\left(\begin{array}{c} 2 \\ 1 \\ 2 \\\end{array}\right)$
* $\frac{8}{9}\left(\begin{array}{c} 2 \\ 1 \\ 2 \\\end{array}\right)$

### plain

The vector projection must be a multiple of $v_1$ since we're projecting $u_2$ onto $v_1$.

### mcq

Now, what is $v_2$?

* $\left(\begin{array}{c} 1 \\ 2 \\ 2 \\\end{array}\right) - \frac{8}{9}\left(\begin{array}{c} 2 \\ 1 \\ 2 \\\end{array}\right)$


### plain

Indeed, this is the orthogonal complement when $u_2$ is projected onto $v_1$. Working out the math, we have:

$$v_2 = \frac{1}{9}\left(\begin{array}{c} -7 \\ 10 \\ 2 \\\end{array}\right)$$

### mcq

Here's a short little task for you before we move on to the next part. Find $v_1 \cdot v_2$

* $\left(\begin{array}{c} 0 \\ 0 \\ 0 \\\end{array}\right)$

### plain

Well, it has to be! They are orthogonal after all.

Anyways, that's the end of the example. Let's step things up a dimension.

### plain

Let's suppose we have a subspace $V$ whose basis are $\{u_1,u_2,u_3\}$ (this time the entire space is more than 3 dimensional)

### plain

Let's try to create an orthogonal basis $\{v_1,v_2,v_3\}$. 

We can let $v_1 = u_1$.

Then, we find $v_2$ by projecting it onto $v_1$ and taking it's orthogonal component.

### mcq

Now, we have $\text{span}\{u_1,u_2\}$ = $\text{span}\{v_1,v_2\}$, and we want to add the third vector.

How do we go get $v_3$ from $u_3$?

- We project $v_3$ onto $v_2$
- We project $v_3$ onto $v_1$
* We project $v_3$ onto $\text{span}\{v_1,v_2\}$

### embed

https://www.desmos.com/3d/tfj5mhx387
- Projection onto subspace 2

### mcq

Let's look at the desmos again. If we let the red vector be $u_3$, then the purple vector is its projection onto the subspace. What should $v_3$ be then?

- purple
- red
* red - purple
- red + purple

### plain

Notice that (red-purple) is orthgoonal the subspace! Hence, it is a good candidate for $v_3$

### mcq

So what should the expression for $v_3$?

- $\frac{u_1\cdot u_3}{|u_1|^2}u_1 + \frac{u_2\cdot u_3}{|u_2|^2}u_2$
- $\frac{v_1\cdot u_3}{|v_1|^2}v_1 + \frac{v_2\cdot u_3}{|v_2|^2}v_2$
- $u_3 - \frac{u_1\cdot u_3}{|u_1|^2}u_1 + \frac{u_2\cdot u_3}{|u_2|^2}u_2$
* $u_3 - \frac{v_1\cdot u_3}{|v_1|^2}v_1 + \frac{v_2\cdot u_3}{|v_2|^2}v_2$


### plain

Firstly, the projection of $u_3$ onto the subspace $\text{span}\{v_1,v_2\}$ is $\frac{v_1\cdot u_3}{|v_1|^2}v_1 + \frac{v_2\cdot u_3}{|v_2|^2}v_2$. We must use $v_1, v_2$ as the basis because they are orthogonal. This formula doesn't work if the basis is not orthogonal.

However, we're not looking for the projection! 

### plain

Recap that previously, we discussed that when vector $w$ is projected onto $u$, it can be expressed as $w_p + w_n$, where $w_p$ is in the same direction as $u$, and $w_n$ is orthogonal to $u$.

In fact, we can generalize this to subspaces too. If $V$ is a subspace, then every vector $w$ can be expressed as $w_p + w_n$ where $w_p$ is in the subspace $V$, and $w_n$ is orthogonal to the subspace $V$.

### plain

Hence, $v_3$ is actually the orthogonal component when $v_3$ is projected onto the subspace $\text{span}\{v_1,v_2\}$.

### plain

In summary, the gist of converting any basis $\{u_1 \dots u_k\}$ for a subspace $V$ to an orthogonal basis $\{v_1 \dots v_k\}$ is as follows:

1. Let $S$ be a set. Let $v_1 = u_1$, and initialize $S = \{v_1\}$
2. Project $u_2$ onto the subspace $\text{span(S)}$. Let the projection be $u_{2p}$
3. Let $v_2 = u_2 - u_{2p}$, i.e. the orthogonal component
4. Insert $v_2$ into $S$
5. Repeat steps 2-4, with $u_3$ all the way to $u_k$.

Finally, we often want an orthonormal basis instead of orthogonal basis. Hence, we have a sixth and final step:

6. Normalize all vectors to be length $1$

### plain

The entire process described above is the gist of the Gram-Schmidt Process. 

### tex
$$\textbf{Gram-Schmidt Process} \\ $$
Let $S=\{\mathbf{u}_1,\mathbf{u}_2,...,\mathbf{u}_k\}$ be a linearly independent set. Let

$\mathbf{v}_1 = \mathbf{u}_1$
$\mathbf{v}_2 = \mathbf{u}_2-\left(\frac{\mathbf{v}_1\cdot\mathbf{u}_2}{\lVert\mathbf{v}_1\rVert^2}\right)\mathbf{v}_1$
$\mathbf{v}_3 =\mathbf{u}_3-\left(\frac{\mathbf{v}_1\cdot\mathbf{u}_3}{\lVert\mathbf{v}_1\rVert^2}\right)\mathbf{v}_1-\left(\frac{\mathbf{v}_2\cdot\mathbf{u}_3}{\lVert\mathbf{v}_2\rVert^2}\right)\mathbf{v}_2$
$\vdots$
$\mathbf{v}_k = \mathbf{u}_k-\left(\frac{\mathbf{v}_1\cdot\mathbf{u}_k}{\lVert\mathbf{v}_1\rVert^2}\right)\mathbf{v}_1-\left(\frac{\mathbf{v}_2\cdot\mathbf{u}_k}{\lVert\mathbf{v}_2\rVert^2}\right)\mathbf{v}_2-\cdots-\left(\frac{\mathbf{v}_{k-1}\cdot \mathbf{u}_k}{\lVert\mathbf{v}_{k-1}\rVert^2}\right)\mathbf{v}_{k-1}.$

<br>

Then $\{\mathbf{v}_1,\mathbf{v}_2,...,\mathbf{v}_k\}$ is an orthogonal set (of nonzero vectors), and hence,
$$\left\{\frac{\mathbf{v}_1}{\lVert\mathbf{v}_1\rVert},\frac{\mathbf{v}_2}{\lVert\mathbf{v}_2\rVert},...,\frac{\mathbf{v}_k}{\lVert\mathbf{v}_k\rVert}\right\}$$
is an orthonormal set such that $\text{span}\{\mathbf{v}_1,...,\mathbf{v}_{k}\}=\text{span}\{\mathbf{u}_k,...,\mathbf{u}_{k}\}$.

### dialogue

There's just one more part for this, so once you've gotten the Gram-Schmidt Process down, let me know :)

- I'm ready!
    - QRFactStart
    
## QRFact QR Factorization

### plain QRFactStart

Let's consider a basis for a subspace $V$ in $R^m$, where the basis is $\{a_1, a_2, \dots, a_n\}$.

Suppose we perform the Gram-Schmidt Process on $\{a_1, a_2, \dots, a_n\}$, and we get $\{q_1, q_2, \dots, q_n\}$ as an orthogonal basis for the same subspace.

### mcq

Consider the vector $a_3$. Which of the following statements are true?

- $a_3$ is a linear combination of $q_1$
- $a_3$ is a linear combination of $q_1, q_2$
* $a_3$ is a linear combination of $q_1, q_2, q_3$

### plain

Let's recap the gram-schmidt process for getting $q_3$:

$$q_3 = a_3 - \frac{q_1\cdot a_3}{|q_1|^2}q_1 + \frac{q_2\cdot a_3}{|q_2|^2}q_2$$

Afterwards, we normalize $q_3$ to be a unit vector.

### plain

Anyways, from the above, we can see that $a_3$ is a linear combination of $q_1, q_2, q_3$.

In general, $a_k$ is a linear combination of $q_1, q_2, \dots, q_k$.

### mcq

Now, let the matrix $Q = \left(q_1 \space \space q_2 \space \space \dots q_n\right)$

Let's suppose that we have some vector $r_1$, such that $a_1 = Qr_1$.

How many non-zero rows must $r_1$ have?

- $0$
* $1$
- $n$

### plain

Recap how matrix vector multiplication works. If we let $r_1 = \left(\begin{array}{c} r_{1,1} \\ r_{2,1} \\ \vdots \ \\ r_{n,1} \\ \end{array}\right)$

When we take $Qr_1$, we are saying we want $r_{1,1}$ times of $q_1$, plus $r_{2,1}$ times of $q_2$, and so on.

### plain 

Since $a_1$ is a linear combination of $q_1$ only, hence we don't need $q_2$ onwards! $r_{2,1}$ onwards are all zero!

### mcq

Now, suppose that we have some vector $r_2$, such that $a_1 = Qr_2$.

How many non-zero rows must $r_2$ have?

- $0$
- $1$
* $2$

### plain

Indeed, by similar logic as above, $a_2$ is dependent only on $q_1, q_2$. Hence, $r_2$ only has the first two rows be non-zero.

Repeating this logic, we have that $r_n$ in general has $n$ non-zero rows from the top.

### mcq

Now, let $R = \left(r_1 \space \space r_2 \space \space \dots r_n\right)$. What type of matrix is $R$?

* Upper Triangular
- Strict Upper Triangular
- Lower Triangular
- Strict Lower Trianglular

### plain

Well, it is upper triangular because the number of non-zero rows from the top decreases as we go down.

### plain

Anyways, let $A = \left(a_1 \space \space a_2 \space \space \dots a_n\right)$

We have that $a_k = Qr_k$ for all $k$

Hence by block multiplication (which can be thought of here as gluing of $a$ and $r$ together), we have $A = QR$.

### plain

This is known as the QR factorization of a matrix

### mcq

Question! Is $Q$ an orthogonal matrix?

- Yes
* No

### dialogue

What are the dimensions of $Q$?

- oh...
    - afterTrick
- $m \times n$
    - afterTrick

### plain afterTrick

Remember that an orthogonal matrix must be a square matrix! In this case, the orignal matrix $A$ may not even be square.

### mcq

However... is it true that $Q^TQ = I$?

* Yes
- No

### plain

The reason is exactly the same as why orthogonal matrices also have this $Q^TQ = I$ property. 

Anyways, here's the definition for QR Factorization:


### tex
$$\textbf{QR Factorization} \\ $$

Given a $m \times n$ matrix A, we may write:

$$\mathbf{A} \space \space= \space \space\begin{pmatrix}\mathbf{a}_1 \space \space\mathbf{a}_2 \space \space\cdots \space \space\mathbf{a}_n\end{pmatrix}$$

$$= \begin{pmatrix}\mathbf{q}_1 \space \space\mathbf{q}_2 \cdots \space \space\mathbf{q}_n\end{pmatrix}\begin{pmatrix}
r_{11} & r_{12} & \cdots & r_{1n}\\
0 & r_{22} & \cdots & r_{2n}\\
\vdots & \vdots & \ddots& \vdots\\
0 & 0 &\cdots & r_{nn}
\end{pmatrix}$$
$$= \space \space \mathbf{QR}$$

for some $m\times n$ matrix $\mathbf{Q}$ with orthonormal columns, and a upper triangular $n\times n$ matrix $\mathbf{R}$.

Because the columns of $\mathbf{Q}$ are orthonormal, $\mathbf{Q}^T\mathbf{Q} = I$

### dialogue

Now, let's address the algorithm for computing QR factorization.

We can get $Q$ from the gram-schmidt process. How do we get $R$?

- We can compute it during the gram-schmidt process
    - QRBadWay
- There must be a better way!
    - QRGoodWay

### plain QRBadWay

Well, it works, but it's pretty tedious to do.

### plain QRGoodWay

There is indeed a better way.

### mcq 

Let's consider the equation $A = QR$.

How do we extract the $R$ out?

- pre-multiply by $Q^{-1}$
* pre-multiply by $Q^T$

### plain

Well, the inverse idea is there. But $Q$ may not even have an inverse as it's not square.

Instead, we can make use of the fact that $\mathbf{Q}^T\mathbf{Q} = I$. Premultipling by $Q^{T}$ on both sides, we have $Q^{T}A = Q^{T}QR = R$.

Hence, to get $R$, we just need to mulitply $Q^{T}$ and $A$! Pretty simple right?


### plain

And that's a wrap! 