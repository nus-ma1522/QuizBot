## ProjectionRecap Recap On Projections


### plain

Let's talk recap about dot products and vector projection first. Let's consider projecting a vector $w$ onto a vector $u$. Try playing with the visualization below and see how it changes!


### embed 

https://www.desmos.com/geometry/wt9btukes9
- Projection of w onto u

### mcq

Based on the above example, when is the dot product zero?

* When they're perpendicular
- When they're in the same direction

### plain

The dot product of $w$ and $u$ can be defined as $w\cdot u =w_1u_1 + w_2u_2 \dots w_nu_n$.

It can also be defined as $w\cdot u =|w||u|\cos\theta$, where $\theta$ is the angle between the two vectors.

### mcq
Let's examine the $\cos\theta$ term. When is $\cos \theta = 0$?

* When the vectors are perpendicular!

### plain

Indeed! When two vectors are perpendicular, their dot product is zero. From now on, the term we will use is <b>orthogonal</b>.

### tex

$$\textbf{Orthogonal Vectors} \\ $$

Vectors $w$ and $u$ are orthogonal if and only if $w \cdot u = 0$

### plain

Let's go back to the discussion of the angle between $w$ and $u$ again.

If $\theta = 0$ or $\theta = 180^\circ$, $w$ and $u$ point in a "100% related" direction, that is $w$ is a multiple of $u$.

If $\theta = 90^\circ$, then they point in a "0% related" direction, that is they are orthogonal to each other.



### plain

Let's head to this desmos page. Drag the red point $W$ around and notice how the vector $w$ can always be expressed as the sum of the green vector $w_p$ and purple vector $w_n$

### embed

https://www.desmos.com/geometry/y2ubn4hqts
- Decomposition of w into 100% related and 0% related components

### plain

Notice that even if we choose a vector $w$ that is neither "0% related" nor "100% related" to $u$, we can decompose $w$ as a sum of a "100% related" (parallel) component, $w_p$, and "0% related" (orthogonal) component, $w_n$!

The green vector $w_p$ is the projection of $w$ onto $u$, and hence it is a multiple of $u$. (the $p$ in $w_p$ stands for projection).

The purple vector $w_n$ is orthogonal to $u$ ($n$ stands for normal, which has a similarish meaning as orthgonal)

### plain

Let's quickly go through some definitions that are important for the next part:

### tex

$$\textbf{Orthogonal Set} \\ $$

A set $S = {u_1, u_2, ..., u_k}$ of vectors in $R_n$ of vectors is orthogonal if $u_i Â· u_j = 0$ for every $i \neq j$

$$\textbf{Orthonormal Set} \\ $$

A set $S = {u_1, u_2, ..., u_k}$ of vectors in $R_n$ of vectors is orthornomal if it is orthogonal and all vectors are unit vectors.

### mcq

Pop quiz! Which of the following statements must be true?

- An orthogonal set $S$ must be linearly independent
* An orthonormal set $S$ must be linearly independent

### plain

Okay okay this is kind of a trick question, so bear with me.

An orthgonal set is almost always linearly independent. After all, if two vectors are orthogonal, they are "0% related", and hence they must be linearly independent. If you picked both as correct, you've got the right intution!

### mcq

The problem here is this. For any vector $w$, what is $\mathbf{0} \cdot w$?

* $\mathbf{0}$!

### plain

Exactly! The problem here is that the zero vector is orthgonal to every other vector. And a set that contains the zero vector cannot be linearly independent.

An orthogonal set can contain the zero vector, and would make it not linearly independent. However, since vectors must be unit vectors in an orthonormal set, the zero vector cannot exist in an orthonormal set.

### dialogue
Anyways, at this point, we have established why we care about whether two vectors are orthogonal - they give us a way to understand if two vectors are "0% related".

When you're ready, let's move on to the next part!

- I'm ready!
    - ProjectionOntoSubspaceStart

## ProjectionOntoSubspace Projection onto Subspace

### mcq ProjectionOntoSubspaceStart

Question! Is $\text{span}\{u\}$ a subspace?

* Yes
- No

### mcq

When we project $w$ onto the vector $u$, in reality we're actually projecting $w$ onto the one-dimensional subspace (a.k.a line).

So... can we project vectors onto a plane? or a higher dimension subspace???

* Yes!
* Get me outta here :<

### plain

Let's take things one step at a time




### plain 

Given two vectors $u_1$,$u_2$, we have a subspace $V$ such that $\{u_1, u_2\}$ forms a basis for $V$. The idea is we want to project the vector $w$ onto the subspace $V$ itself.

### mcq

Let $w_p$ be the projection onto the subspace $V$. $w_{p1}$ be the projection of $w$ onto $u_1$, and $w_{p2}$ be the projection of $w$ onto $u_2$. 

Use your visualization skills a little bit. Is it true that $w_p = w_{p1} +w_{p2}$? That is, the projection onto the subspace is the sum of the projection onto each of the basis vectors?

- Yes
* No

### plain

Okay that might have been quite hard to visualize, so let's bring out the desmos again.

### embed

https://www.desmos.com/3d/9vcs8awrit
- Projection onto subspace 1

### mcq

Anyways, why is $w_p \neq w_{p1} +w_{p2}$?

* Because we're double counting the part of $w$ that is related to both $u_1$ and $u_2$

### plain

Exactly! The issue is here is that $u_1$ and $u_2$ are still not "0% related", which means that we can potentially double count the component that are related to both $u_1$ and $u_2$.

### mcq

How do you think we can fix this?

* Make $u_2$ orthogonal to $u_1$!

### plain

Exactly! I've skipped some steps, but I have replaced $u_2$ with another vector such that $u_1, u_2$ is still a basis for $V$, but now $u_2$ and $u_1$ are orthogonal. 

### embed

https://www.desmos.com/3d/tfj5mhx387
- Projection onto subspace 2

### mcq

Now, is it true that $w_p = w_{p1} +w_{p2}$? 

* Yes!

### plain

Indeed, the fact that $u_1$ and $u_2$ are orthogonal to one another is an extremely helpful property! Because $\{u_1, u_2\}$ is an orthogonal set, and is a basis for $V$, we can call it an <b>Orthogonal Basis</b> of $V$. 

Furthermore, if $u_1$, $u_2$ were unit vectors, then we can also call them and <b>Orthonormal Basis</b> of $V$.

### mcq

Here's a short understanding check. For $\mathbb{R}^n$

* The standard basis is an orthogonal basis of $\mathbb{R}^n$
* The standard basis is an orthonormal basis of $\mathbb{R}^n$

### plain

Indeed, the standard basis consist of unit vectors all orthogonal to one another.

In fact, if you plot the vectors of any orthonormal basis, you will find that it looks like the standard basis, just that it's rotated and/or reflected.

### dialogue

I hope this section highlights even further why we care about orthogonality and orthogonal bases. 

The projection onto a subspace is a quick teaser for sections 5.3 and 5.4 where we learn how to actually compute the projection onto a subspace. 

But for now, let's return to the last part of this interactive to tie up some loose ends. Let me know when you're ready!

- I'm ready!
    - VectorProjectionStart

## VectorProjection Projection Computation

### plain VectorProjectionStart

This whole time, we talked about projecting a vector $w$ onto $u$ without showing how to compute it. So let's quickly walk through it and highlight what the formula means

### mcq

Firstly, let $w_p$ be the projection of $w$ onto $u$. $w_p$ is a scalar multiple of:

- $w$
* $u$
- Neither?

### plain

If at any point you're confused, refer back to <a target="_blank" rel="noopener noreferrer" href="https://www.desmos.com/geometry/wt9btukes9">this desmos page.</a>

$w_p$ is a scalar multiple of $u$ as it is projected onto $u$.

To get $w_p$, we want the length of $w_p$, and the direction of $w_p$. 

### plain

First, the direction. Ideally, we want a unit vector for that, and hence we use $\frac{1}{|u|}u$.

Next, the length of $w_p$. Since $\theta$ is the angle between $w$ and $u$, the length of $w_p$ is $w\cos\theta$

We know that $u \cdot w = |u||w|\cos\theta \implies |w|\cos\theta = \frac{u\cdot w}{|u|}$

Putting everything together, we have that $w_p = (\frac{u\cdot w}{|u|})(\frac{1}{|u|}u) = \frac{u\cdot w}{|u|^2}u$

### dialogue

We have one more loose end after this. Whenever you're ready!

- Bring me to <b>Orthogonal Matrices</b>!
    - OrthogonalMatricesStart

## OrthogonalMatrices Orthogonal Matrices

### plain OrthogonalMatricesStart


Let's consider an orthonormal basis $S = \{u_1, u_2, u_3\}$ for $\mathbb{R}^3$. Let's denote $u_1 = \left(\begin{array}{c} a \\ b \\ c \\\end{array}\right)$, $u_2 = \left(\begin{array}{c} d \\ e \\ f \\\end{array}\right)$, $u_3 = \left(\begin{array}{c} g \\ h \\ i \\\end{array}\right)$

Now, let's make a matrix $Q = (u_1 \space \space u_2 \space \space u_3) = \left(\begin{array}{c c c} a & d & g \\ b & e & h \\ c & f & i \\\end{array}\right)$

The transpose of $Q$ is as follows: $Q^T = (u_1 \space \space u_2 \space \space u_3) = \left(\begin{array}{c c c} a & b & c \\ d & e & f \\ g & h & i \\\end{array}\right)$

I know the transpose here is kind of out of nowhere, so bear with me for a bit we'll see where this takes us.

### mcq plain

Let's try to evaluate $Q^Tu_1$

$$Q^Tu_1 = \left(\begin{array}{c c c} a & b & c \\ d & e & f \\ g & h & i \\\end{array}\right)\left(\begin{array}{c} a \\ b \\ c \\\end{array}\right)$$

$Q^Tu_1$ is a vector of 3 rows. How do we get the first entry of $Qu_1$?

* $a \cdot a + b \cdot b + c \cdot c$
* $u_1 \cdot u_1$
* $1$

### plain

You'll realize that the first entry is the dot product of $u_1$ and itself, and since $u_1$ is a unit vector, it must give $1$!

### mcq

Next, what about the second entry of $Q^Tu_1$?

* $a \cdot d + b \cdot e + c \cdot f$
* $u_1 \cdot u_2$
* $0$

### plain

This time, $u_1 \cdot u_2$ is $0$ because $u_1$ and $u_2$ are orthogonal! Similarly, we can say that the third entry is also $0$.

Hence, 

$$Q^Tu_1 = \left(\begin{array}{c c c} a & b & c \\ d & e & f \\ g & h & i \\\end{array}\right)\left(\begin{array}{c} a \\ b \\ c \\\end{array}\right) = \left(\begin{array}{c} 1 \\ 0 \\ 0 \\\end{array}\right)$$

### mcq

Now, try to work out what $Q^Tu_2$ should be:

* $\left(\begin{array}{c} 1 \\ 0 \\ 0 \\\end{array}\right)$
* $\left(\begin{array}{c} 0 \\ 1 \\ 0 \\\end{array}\right)$
* $\left(\begin{array}{c} 0 \\ 0 \\ 0 \\\end{array}\right)$

### plain

And by a similar logic, $Qu_3$ should give us $\left(\begin{array}{c} 0 \\ 0 \\ 1 \\\end{array}\right)$.

### plain

Putting all of this together, we have:

$$Q^T(u_1 \space \space u_2 \space \space u_3) = Q^TQ = \left(\begin{array}{c c c} 1 & 0 & 0 \\ 0 & 1 & 0 \\ 0 & 0 & 1 \\\end{array}\right)$$

And voila! This also means that $Q^T = Q^{-1}$, i.e. $Q$'s inverse is also its transpose. 

Matrices such as $Q$ are called **Orthogonal Matrices** 

### tex

$$\textbf{Orthogonal Matrix} \\ $$

For a square matrix $Q$ of size $n$ to be orthogonal, there are three equivalent definitions:

1. $Q^TQ = I$
2. The columns of $Q$ form an orthonormal basis for $\mathbb{R}^n$
3. The rows of $Q$ form an orthonormal basis for $\mathbb{R}^n$

### plain

That's a wrap! We will continue exploring the projection onto a subspace in the next interactive :)






